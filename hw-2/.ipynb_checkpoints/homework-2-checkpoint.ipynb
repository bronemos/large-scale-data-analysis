{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homework set 2\n",
    "## Large Scale Data Analysis / Aalto University, Spring 2022\n",
    "\n",
    "This homework set consists of <b>3 questions</b>. You will implement the Bagging algorithm, Random Forest and AdaBoost.M1 by yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returned by :  My name, student number "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you wish to use Matlab for solving these problems, then that is fine as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "Note: you do not need any other packages, so if you import something else, please specify why you need those packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl \n",
    "import pandas as pd \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "def generate_data(n, p):\n",
    "    x = np.random.normal(size=(n, p))\n",
    "    sdX = np.sum(x ** 2, axis=1)\n",
    "    c = chi2.ppf(q=0.5, df=p)\n",
    "    y = np.ones(n)\n",
    "    y[sdX <= c] = -1\n",
    "    return x, y\n",
    "\n",
    "N = 2000\n",
    "Nt = 10000\n",
    "p = 10\n",
    "np.random.seed(0)\n",
    "X, y = generate_data(N, p)\n",
    "Xt, yt = generate_data(Nt, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 1\n",
    "In this problem you will implement the bagging algorithm in the \n",
    "binary classification problem and use it to redo the Figure 3.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare your results with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.72 s, sys: 7.68 ms, total: 2.73 s\n",
      "Wall time: 2.74 s\n",
      "Sklearn Bagging error rate  : 13.82%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "%time sk_Bag = BaggingClassifier(n_estimators=200).fit(X,y)\n",
    "errBag = 1-accuracy_score(yt,sk_Bag.predict(Xt))\n",
    "print(\"Sklearn Bagging error rate  : {:5.2f}%\".format(100*errBag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 (a) \n",
    "The bagging algorithm for classification is described in algorithm 3.1 of lecture notes. Write this function yourself by writing a function named <code>MyBagging</code> having outputs a list  <code>Trees</code> that contains the <code>B</code> bagged decision trees and an array or list  <code>err</code> that contains the <code>B</code> out-of-bag (OOB) training errors when a new tree is added.\n",
    "\n",
    "_Note:_ \n",
    " You shoud use <code>DecisionTreeClassifier</code> to compute the classification decision tree (with default options). You are not allowed to use <code>BaggingClassifier</code> in the implemention of <code>MyBagging</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyBagging(X, y, B):\n",
    "    # your code comes here\n",
    "    \n",
    "    return Trees, Err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 (b)\n",
    "Write a function <code>PredictBagging</code> that computes the predicted class labels <code>yhat</code> for input data <code>X</code> for each bagged tree as well as the error rate <code>Err</code> at each iteration, i.e., when a new tree is added, given the true labels <code>y</code>. The input  <code>Trees</code> is the output from <code>MyBagging</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictBagging(Trees, X, y):\n",
    "    \"\"\"\n",
    "      PredictBagging\n",
    "      \n",
    "      params:\n",
    "        Trees          A list of Bagged trees\n",
    "        X,y            test data \n",
    "\n",
    "    \"\"\"    \n",
    "  \n",
    "    # your code comes here\n",
    "    \n",
    "    return yhat, Err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 (c)\n",
    "Use the functions you made in part 1(a) and 1(b) to redo the Figure 3.3a in the lecture notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 200 # Use B=200 bagged trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to reproduce Figure 3.3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 2\n",
    "In this problem you will implement the random forest algorithm in the  binary classification problem and use it to redo the Figure 3.3b in the lecture notes.\n",
    "\n",
    "_Hint_: Again you shoud use <code>DecisionTreeClassifier</code> to compute the classification decision tree. You can use the <code>PredictBagging</code> function to compute the predicted class labels for  an input data <code>X</code>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare with the scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 698 ms, sys: 6.01 ms, total: 704 ms\n",
      "Wall time: 703 ms\n",
      "Sklearn Random Forest error rate  : 12.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "%time sk_RF = RandomForestClassifier(n_estimators=200,max_features=2,min_samples_leaf=3).fit(X,y)\n",
    "errRF = 1-accuracy_score(yt,sk_RF.predict(Xt))\n",
    "print(\"Sklearn Random Forest error rate  : {:5.2f}\".format(100*errRF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b>: scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class. In <code>PredictBagging</code>, you uses the original method, where each tree gets a single vote. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2 (a)\n",
    "Implement the random forest algorithm for classification described in algorithm 3.2 by yourself by writing a function named <code>MyRandomForest</code>. The outputs of this function  are an object  <code>Trees</code> which contains the _B_ decision trees classifiers and a  vector or list <code>err</code> that contains the _B_ out-of-bag (OOB) training error when a new tree is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyRandomForest(X, y, B, d, nmin):\n",
    "    \"\"\"\n",
    "      MyRandomForest function\n",
    "      \n",
    "      params:\n",
    "        X,y            training data \n",
    "        B              the number of learners\n",
    "        d              number of features in each split\n",
    "        nmin           minimum node size\n",
    "    \"\"\"    \n",
    "    # your code comes here\n",
    "          \n",
    "    \n",
    "    return Trees, Err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (b)\n",
    "Use the functions you have made to redo the Figure 3.3b. Use the following parameter values: <code>d=2, nmin=3</code>,  <code>B=200</code> as given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 200\n",
    "d = 2\n",
    "nmin = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Question 3\n",
    "In this problem you will implement the Adaboost.M1 algorithm and use it  to redo the Figure 4.1a.  \n",
    "_Note_: Again you should use <code>DecisionTreeClassifier</code> to compute the stumps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare your algorithm's perforformance to Sklearn (you should get the exactly same result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.59 s, sys: 6.33 ms, total: 1.6 s\n",
      "Wall time: 1.6 s\n",
      "Sklearn AdaBoost.M1 error rate  : 10.25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "%time sk_AdaM1 = AdaBoostClassifier(n_estimators=600,algorithm='SAMME').fit(X,y)\n",
    "errAdaM1 = 1-accuracy_score(yt,sk_AdaM1.predict(Xt))\n",
    "print(\"Sklearn AdaBoost.M1 error rate  : {:5.2f}\".format(100*errAdaM1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (a)\n",
    "Implement the Adaboost.M1 algorithm described in algorithm 4.1 using stumps (classification decision trees with two terminal nodes) as the base learner. Write a function <code>MyAdaBoostM1</code>, whose outputs are  a list <code>G</code> that contains the <code>M</code> trees and a list or array <code>alpha</code> that contains weights of each boosting iteration.  Parameter <code>node</code> is the number of leaves you wish to use in your base learner. Default is <code>node=2</code>, so using stumps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyAdaBoostM1(X, y, M, node=2):\n",
    "\n",
    "    # your code here \n",
    "    \n",
    "    return G, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "Write a function <code>PredictAdaBoostM1</code> that computes the predicted data labels and error rate at each boosting iteration for a given input test data. The inputs of the function are <code>G</code> and  <code>alpha</code> which are the outputs from <code>MyAdaBoostM1</code> as well as <code>X</code> and <code>y</code> which are the input data of features and class labels. The outputs are the predicted labels <code>yhat</code> and the error rates <code>Err</code> at each boosting iteration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictAdaBoostM1(G, alpha, X, y):\n",
    "\n",
    "    # your code here \n",
    "    \n",
    "    return yhat, Err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)\n",
    "Use the functions you made in a) and b) to redo the Figure 4.1a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 600 # use 600 boosting iterations\n",
    "node = 2 # for stumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
